Complete this document and upload along with your prediction results and your code.

### Method Name ###

Word Embedding + LSTM


### Representation of sentence ###
Use up to 3 sentences to describe how you obtained the representation/features of each ciphertext sentence. E.g., bag-of-words? trained a word2vec or fasttext on all sentences from scratch?

I used Keras tokenizer to tokenize or vectorize the text corpus. The tokens of the text corpus were converted into a sequence of integers using the texts_to_sequence method. The vocabulary of texts was updated using the fit_on_texts method. The sequence was also padded by having the maximum length of the sequence as a hyperparameter.


### Classifier ###
Use up to 5 sentences to describe how you implemented your classifier. What encoder did you use and what was the learning objective?

I used the LSTM implemented using Keras sequential model to classify the ciphertext sentences. The shape of the training data is the input to the embedding layer. It was followed by 2 LSTM layers with a 'tanh' activation function and dropouts. Since it is a binary classification problem, a dense output layer with 2 neurons and a softmax activation function made the job easier. We have already encoded the input for LSTM using the texts_to_sequence method. The learning objective was to maximize the validation accuracy.

### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting to the leaderboard? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? How did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?

I used the accuracy metric on the dev set to validate my model's performance. I also saw the distribution of 1's and 0's in the dev set and compared it to my results to verify that the model doesn't skew. Hyperparameters : max_words = 70000,  max_length_of_seq = 250, emb_dim = 100, dropout = 0.25, optimizer = Adam, num_epochs = 10 batch_size = 32. The training was terminated using the early stopping method with patience = 3(stop the training when there is no improvement in validation accuracy after 3 consecutive epochs).

### Other methods ###
Did you try other methods than the submitted one?

Bag of words + FFNN
SVM

### Packages ###
List the key python packages you have used in this assignment.

Keras, Tensorflow, Sklearn, numpy, pandas

